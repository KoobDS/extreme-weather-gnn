{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2193ea7f",
   "metadata": {},
   "source": [
    "1. Processes individual raw weather station files (adapted from Vasu's code). \n",
    "2. Merges them into a unified dataset. \n",
    "3. Runs PCA to retain 95% of explained variance and reports dropped features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750fdc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: BUFFALO.csv\n",
      "Processed: CHICAGO.csv\n",
      "Processed: CLEVELAND.csv\n",
      "Processed: MILWAUKEE.csv\n",
      "Processed: TOLEDO.csv\n",
      "Processed: DETROIT.csv\n",
      "Processed: ERIE.csv\n",
      "Processed: ROCHESTER.csv\n",
      "Processed: GREEN BAY.csv\n",
      "Processed: MARQUETTE.csv\n",
      "Processed: APPLETON.csv\n",
      "Processed: GRAND RAPIDS.csv\n",
      "Processed: SAULT STE MARIE.csv\n",
      "Processed: SOUTH BEND.csv\n",
      "Processed: SYRACUSE.csv\n",
      "Processed: ALPENA.csv\n",
      "Processed: FORT WAYNE.csv\n",
      "Processed: HOUGHTON LAKE.csv\n",
      "Processed: LANSING.csv\n",
      "Processed: SAGINAW.csv\n",
      "Processed: HIBBING.csv\n",
      "Processed: MONTELLO.csv\n",
      "Processed: MUSKEGON.csv\n",
      "Processed: TRAVERSE CITY.csv\n",
      "Processed: WATERTOWN.csv\n",
      "Processed: CHEBOYGAN.csv\n",
      "Processed: HOUGHTON.csv\n",
      "Processed: IRONWOOD.csv\n",
      "Processed: MANISTEE.csv\n",
      "Processed: MARINETTE.csv\n",
      "Processed: AUBURN.csv\n",
      "Processed: BAD AXE.csv\n",
      "Processed: ESCANABA.csv\n",
      "Processed: GRAND MARAIS.csv\n",
      "Processed: SUPERIOR.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from constants import DROP_COLUMNS, CITY_NAMES, DATE_RANGE, NUMERICAL_COLUMNS, COLUMN_ORDER\n",
    "\n",
    "# 1. Process raw NOAA files and split per station\n",
    "def process_data(file_basename):\n",
    "    df = pd.read_csv(f\"data/raw/{file_basename}.csv\")\n",
    "    df = df.drop(columns=DROP_COLUMNS, errors=\"ignore\")\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "\n",
    "    groups = df.groupby(\"NAME\")\n",
    "\n",
    "    for station_name, station_data in groups:\n",
    "        city_found = False\n",
    "        for city in CITY_NAMES:\n",
    "            if city in station_name:\n",
    "                station_name = city\n",
    "                station_data[\"NAME\"] = city\n",
    "                CITY_NAMES.remove(city)\n",
    "                city_found = True\n",
    "                break\n",
    "\n",
    "        if not city_found:\n",
    "            print(f\"Skipping unknown station: {station_name}\")\n",
    "            continue\n",
    "\n",
    "        station_data.set_index(\"DATE\", inplace=True)\n",
    "        station_data = station_data.sort_index().reindex(DATE_RANGE)\n",
    "        station_data.index.name = \"DATE\"  # Ensure DATE is saved correctly\n",
    "\n",
    "        # Restore static columns\n",
    "        station_data[\"NAME\"] = station_data[\"NAME\"].fillna(station_name)\n",
    "        station_data[\"LATITUDE\"] = station_data[\"LATITUDE\"].fillna(station_data[\"LATITUDE\"].mode()[0])\n",
    "        station_data[\"LONGITUDE\"] = station_data[\"LONGITUDE\"].fillna(station_data[\"LONGITUDE\"].mode()[0])\n",
    "        station_data[\"ELEVATION\"] = station_data[\"ELEVATION\"].fillna(station_data[\"ELEVATION\"].mode()[0])\n",
    "\n",
    "        for col in NUMERICAL_COLUMNS:\n",
    "            if col not in station_data.columns:\n",
    "                station_data[col] = np.nan\n",
    "            station_data[col] = station_data[col].fillna(station_data[col].mean())\n",
    "\n",
    "        station_data = station_data[COLUMN_ORDER]\n",
    "\n",
    "        os.makedirs(\"data/processed\", exist_ok=True)\n",
    "        station_data.reset_index().to_csv(f\"data/processed/{station_name}.csv\", index=False)\n",
    "        print(f\"Processed: {station_name}.csv\")\n",
    "\n",
    "# Run all acronym files\n",
    "files = [\"CMCTB\", \"RGEDM\", \"SGASS\", \"FSALH\", \"MHWTM\", \"CHIMM\", \"SBGAE\"]\n",
    "for f in files:\n",
    "    process_data(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1572f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Merge all processed station files\n",
    "def merge_processed_data():\n",
    "    dfs = []\n",
    "    for f in os.listdir(\"data/processed\"):\n",
    "        if f.endswith(\".csv\"):\n",
    "            try:\n",
    "                df = pd.read_csv(f\"data/processed/{f}\")\n",
    "                if \"DATE\" not in df.columns:\n",
    "                    print(f\"Skipping {f} â€” no DATE column.\")\n",
    "                    continue\n",
    "                df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "                df[\"STATION\"] = f.replace(\".csv\", \"\")\n",
    "                df.set_index(\"DATE\", inplace=True)\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {f}: {e}\")\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No valid processed CSV files found.\")\n",
    "    merged = pd.concat(dfs)\n",
    "    merged.sort_index(inplace=True)\n",
    "    return merged\n",
    "\n",
    "# Merge and store\n",
    "merged_df = merge_processed_data()\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a21407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 12 components to retain 95.0% variance\n",
      "Saved PCA-reduced dataset with original column names to data/merged_pca.csv\n"
     ]
    }
   ],
    "source": [
    "# 3. PCA reduction (dynamic threshold)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def run_pca(df, variance_threshold=0.95):\n",
    "    station_meta = [\"STATION\", \"NAME\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"]\n",
    "    meta_df = df[station_meta]\n",
    "    features = df.drop(columns=station_meta, errors=\"ignore\")\n",
    "    features = features.dropna(axis=1, how=\"all\")\n",
    "    features = features.fillna(0.0)\n",
    "    #print(features.columns)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(features)\n",
    "    print(X.shape)\n",
    "\n",
    "    full_pca = PCA()\n",
    "    full_pca.fit(X)\n",
    "    \n",
    "\n",
    "    cumulative = full_pca.explained_variance_ratio_.cumsum()\n",
    "    n_components = np.argmax(cumulative >= variance_threshold) + 1\n",
    "    print(f\"Selected {n_components} components to retain {variance_threshold*100:.1f}% variance\")\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "\n",
    "    pca_feature_names = features.columns[:n_components]\n",
    "    pca_df = pd.DataFrame(X_reduced, index=features.index, columns=pca_feature_names)\n",
    "    final_df = pd.concat([meta_df, pca_df], axis=1)\n",
    "    #print(final_df.head(3))\n",
    "    print(pca_df.columns)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=3, random_state=42) \n",
    "    cluster_labels = kmeans.fit_predict(pca_df)\n",
    "    pca_df['cluster'] = cluster_labels\n",
    "    fig, ax = plt.subplots() \n",
    "    plot = ax.scatter(pca_df.iloc[:, 0], pca_df.iloc[:, 1], c=pca_df['cluster'],\n",
    "    cmap='tab10',\n",
    "    alpha=0.7)\n",
    "    ax.set_xlabel(f'PCA 1 ({full_variance[0] * 100:.2f}%)')\n",
    "    ax.set_ylabel(f'PCA 2 ({full_variance[1] * 100:.2f}%)')\n",
    "    ax.set_title(f\"PCA\")\n",
    "    \n",
    "    legend_labels = [f'Cluster {i}' for i in sorted(pca_df['cluster'].unique())]\n",
    "    handles = [Line2D([], [], color='w', marker='o', label=label,\n",
    "                         markerfacecolor=plot.cmap(plot.norm(i)), markersize=10)\n",
    "           for i, label in enumerate(legend_labels)]\n",
    "    ax.legend(handles=handles, title=\"Clusters\")\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    contributions = pd.DataFrame(\n",
    "    full_pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(full_pca.n_components_)],\n",
    "    index=features.columns\n",
    "    )\n",
    "    pc1 = contributions['PC1'].abs().sort_values(ascending=False).head(5)\n",
    "    pc2 = contributions['PC2'].abs().sort_values(ascending=False).head(5)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    contributions[['PC1', 'PC2']].plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title(\"Contributions for PCA\")\n",
    "    plt.ylabel(\"Contributions\")\n",
    "    plt.xlabel(\"Climate Variables\")\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"Contributors to PC1:\\n\", pc1)\n",
    "    print(\"\\nContributors to PC2:\\n\", pc2)\n",
    "    \n",
    "    return final_df, n_components, pca, full_pca.explained_variance_ratio_\n",
    "\n",
    "# Run PCA\n",
    "final_df, n_components_used, pca_model, full_variance = run_pca(merged_df)\n",
    "final_df.to_csv(\"data/merged_pca.csv\")\n",
    "print(\"Saved PCA-reduced dataset with original column names to data/merged_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a5e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the PCA-transformed dataset and print explained variance by component\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Load full merged dataset (before PCA)\n",
    "# merged_df = pd.read_csv(\"data/merged_full.csv\", parse_dates=[\"DATE\"])\n",
    "# merged_df.set_index(\"DATE\", inplace=True)\n",
    "\n",
    "# # Extract only numerical features\n",
    "# station_meta = [\"STATION\", \"NAME\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"]\n",
    "# features = merged_df.drop(columns=station_meta, errors=\"ignore\")\n",
    "# features = features.dropna(axis=1, how=\"all\")\n",
    "# features = features.fillna(0.0)\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(features)\n",
    "\n",
    "# # Fit PCA on all components\n",
    "# pca = PCA()\n",
    "# X_pca = pca.fit_transform(X)\n",
    "\n",
    "# # Store explained variance\n",
    "# explained_variance = pd.DataFrame({\n",
    "#     \"Component\": [f\"PC{i+1}\" for i in range(len(pca.explained_variance_ratio_))],\n",
    "#     \"ExplainedVariance\": pca.explained_variance_ratio_,\n",
    "#     \"CumulativeVariance\": pca.explained_variance_ratio_.cumsum()\n",
    "# })\n",
    "\n",
    "# # Save and print summary\n",
    "# explained_variance.to_csv(\"data/pca_variance_full_components.csv\", index=False)\n",
    "# print(\"Saved explained variance for all PCA components to data/pca_variance_full_components.csv\")\n",
    "# print(explained_variance.head(40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
