{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2018eb6e",
   "metadata": {},
   "source": [
    "# Weekly Re‑engineering & Compound Tagging\n",
    "1. Drop old COMPOUND_* columns.\n",
    "2. Aggregate features Mon‑Sun weeks (start Monday 1995‑01‑02).\n",
    "3. Compute `EXT_*` weekly counts by summing.\n",
    "4. Tag new compound events using `PREDEF` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f759f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re, os, math\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_PATH = \"data/merged_pca_11_cmp_7d.csv\"\n",
    "OUT_PATH = \"data/weekly_dataset.csv\"\n",
    "\n",
    "# Load daily data\n",
    "df = pd.read_csv(RAW_PATH, parse_dates=[\"DATE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c1226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations after drop → 33\n"
     ]
    }
   ],
   "source": [
    "# 1. Filter to full weeks starting Monday 1995‑01‑02\n",
    "\n",
    "df = df[df[\"DATE\"] >= \"1995-01-02\"]\n",
    "df.set_index(\"DATE\", inplace=True)\n",
    "\n",
    "# Drop noisy stations\n",
    "BAD_STATIONS = [\"AUBURN\", \"ESCANABA\"]\n",
    "df = df[~df.STATION.isin(BAD_STATIONS)]\n",
    "print(\"Stations after drop →\", df.STATION.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca5223f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly shape (51678, 30)\n"
     ]
    }
   ],
   "source": [
    "# 2. Weekly aggregation spec\n",
    "\n",
    "META = [\"STATION\", \"NAME\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"]\n",
    "agg = {\n",
    "    # mean\n",
    "    \"ACSH\":\"mean\",\"AWND\":\"mean\",\"SNWD\":\"mean\",\"TAVG\":\"mean\",\"TSUN\":\"mean\",\"WDF5\":\"mean\",\"WDFG\":\"mean\",\n",
    "    # sum\n",
    "    \"PRCP\":\"sum\",\"SNOW\":\"sum\",\n",
    "    # extremes\n",
    "    \"TMAX\":\"max\",\"TMIN\":\"min\",\n",
    "}\n",
    "# EXT columns\n",
    "ext_cols = [c for c in df.columns if c.startswith(\"EXT_\")]\n",
    "agg.update({c:\"sum\" for c in ext_cols})\n",
    "\n",
    "weekly = (df.groupby(\"STATION\")\n",
    "            .resample(\"W-MON\", label=\"left\", closed=\"left\")\n",
    "            .agg(agg))\n",
    "weekly.reset_index(inplace=True)\n",
    "weekly.rename(columns={\"DATE\":\"WEEK_START\"}, inplace=True)\n",
    "print(\"Weekly shape\", weekly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13fc0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compound definitions\n",
    "\n",
    "PREDEF = {\n",
    "    \"HEAT_DRY\":[\"EXT_TMAX_HOT\"],\n",
    "    \"HEAT_WIND\":[\"EXT_TMAX_HOT\",\"EXT_AWND_GALE\"],\n",
    "    \"HOT_CLEAR_SKY\":[\"EXT_TMAX_HOT\",\"EXT_ACSH_CLEAR\"],\n",
    "    \"STORM_RAIN_WIND\":[\"EXT_PRCP_P95\",\"EXT_WSFG_DAMG\"],\n",
    "    \"BLIZZARD\":[\"EXT_SNOW_P95\",\"EXT_AWND_GALE\"],\n",
    "    \"RAIN_ON_SNOW\":[\"EXT_PRCP_P95\",\"SNWD_P90_LAG1\",\"TMAX_ABOVE0\"],\n",
    "    \"THAW_FREEZE\":[\"TMAX_HOT_T\",\"TMIN_FROST_TPLUS1\"],\n",
    "    \"BACK2BACK_RAIN\":[\"EXT_PRCP_P95\"],\n",
    "}\n",
    "\n",
    "for tag, conds in PREDEF.items():\n",
    "    for c in conds:\n",
    "        if c not in weekly.columns:\n",
    "            weekly[c] = 0\n",
    "    weekly[f\"COMPOUND_{tag}\"] = (weekly[conds] > 0).all(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e882fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Shift compound tags to next week & drop last week rows\n",
    "\n",
    "comp_cols = [c for c in weekly.columns if c.startswith(\"COMPOUND_\")]\n",
    "weekly.sort_values([\"STATION\",\"WEEK_START\"], inplace=True)\n",
    "for c in comp_cols:\n",
    "    weekly[f\"{c}_next\"] = weekly.groupby(\"STATION\")[c].shift(-1)\n",
    "weekly.dropna(subset=[f\"{c}_next\" for c in comp_cols], inplace=True)\n",
    "\n",
    "# mark splits: 20% stations as validation\n",
    "sts = weekly.STATION.unique()\n",
    "val_sts = np.random.RandomState(1).choice(sts, size=math.ceil(0.2*len(sts)), replace=False)\n",
    "weekly[\"SPLIT\"] = np.where(weekly.STATION.isin(val_sts), \"val\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a76ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/weekly_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# 5. Save\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "weekly.to_csv(OUT_PATH, index=False)\n",
    "print(\"Saved\", OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df93f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 total NaNs\n",
      "['BUFFALO' 'HOUGHTON' 'MARINETTE' 'MARQUETTE' 'SAULT STE MARIE'\n",
      " 'SOUTH BEND' 'TRAVERSE CITY']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "import pandas as pd\n",
    "w = pd.read_csv(\"data/weekly_dataset.csv\")\n",
    "print(w.isna().sum().sum(), \"total NaNs\")\n",
    "print(w[w.SPLIT==\"val\"].STATION.unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
